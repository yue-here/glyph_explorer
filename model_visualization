digraph {
	graph [size="24.3,24.3"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2169072070720 [label="
 (1, 1, 64, 64)" fillcolor=darkolivegreen1]
	2168747558608 [label=SigmoidBackward0]
	2168747559712 -> 2168747558608
	2168747559712 [label=ConvolutionBackward0]
	2168747557024 -> 2168747559712
	2168747557024 [label=ReluBackward0]
	2168747560672 -> 2168747557024
	2168747560672 [label=ConvolutionBackward0]
	2168221943216 -> 2168747560672
	2168221943216 [label=ReluBackward0]
	2168221943360 -> 2168221943216
	2168221943360 [label=ConvolutionBackward0]
	2168221943456 -> 2168221943360
	2168221943456 [label=ReluBackward0]
	2168221944704 -> 2168221943456
	2168221944704 [label=ConvolutionBackward0]
	2168221943600 -> 2168221944704
	2168221943600 [label=ViewBackward0]
	2168747663808 -> 2168221943600
	2168747663808 [label=ReluBackward0]
	2168747663472 -> 2168747663808
	2168747663472 [label=AddmmBackward0]
	2168747664192 -> 2168747663472
	2167409789264 [label="fc2.bias
 (4096)" fillcolor=lightblue]
	2167409789264 -> 2168747664192
	2168747664192 [label=AccumulateGrad]
	2168747663424 -> 2168747663472
	2168747663424 [label=AddBackward0]
	2168747664336 -> 2168747663424
	2168747664336 [label=AddmmBackward0]
	2168747664240 -> 2168747664336
	2167409788304 [label="fc_mu.bias
 (256)" fillcolor=lightblue]
	2167409788304 -> 2168747664240
	2168747664240 [label=AccumulateGrad]
	2168747664000 -> 2168747664336
	2168747664000 [label=ReluBackward0]
	2168747664672 -> 2168747664000
	2168747664672 [label=AddmmBackward0]
	2168747663856 -> 2168747664672
	2167409790304 [label="fc1.bias
 (512)" fillcolor=lightblue]
	2167409790304 -> 2168747663856
	2168747663856 [label=AccumulateGrad]
	2168747663520 -> 2168747664672
	2168747663520 [label=ViewBackward0]
	2168747664816 -> 2168747663520
	2168747664816 [label=ReluBackward0]
	2168747664720 -> 2168747664816
	2168747664720 [label=ConvolutionBackward0]
	2168747664624 -> 2168747664720
	2168747664624 [label=ReluBackward0]
	2168747665056 -> 2168747664624
	2168747665056 [label=ConvolutionBackward0]
	2168747665200 -> 2168747665056
	2168747665200 [label=ReluBackward0]
	2168747664912 -> 2168747665200
	2168747664912 [label=ConvolutionBackward0]
	2168747665728 -> 2168747664912
	2168747665728 [label=ReluBackward0]
	2168747665632 -> 2168747665728
	2168747665632 [label=ConvolutionBackward0]
	2168747665488 -> 2168747665632
	2167409765568 [label="conv1.weight
 (32, 1, 4, 4)" fillcolor=lightblue]
	2167409765568 -> 2168747665488
	2168747665488 [label=AccumulateGrad]
	2168747665392 -> 2168747665632
	2167409765648 [label="conv1.bias
 (32)" fillcolor=lightblue]
	2167409765648 -> 2168747665392
	2168747665392 [label=AccumulateGrad]
	2168747664960 -> 2168747664912
	2167409765728 [label="conv2.weight
 (64, 32, 4, 4)" fillcolor=lightblue]
	2167409765728 -> 2168747664960
	2168747664960 [label=AccumulateGrad]
	2168747665344 -> 2168747664912
	2167409765808 [label="conv2.bias
 (64)" fillcolor=lightblue]
	2167409765808 -> 2168747665344
	2168747665344 [label=AccumulateGrad]
	2168747665248 -> 2168747665056
	2167409766048 [label="conv3.weight
 (128, 64, 4, 4)" fillcolor=lightblue]
	2167409766048 -> 2168747665248
	2168747665248 [label=AccumulateGrad]
	2168747664144 -> 2168747665056
	2167409765888 [label="conv3.bias
 (128)" fillcolor=lightblue]
	2167409765888 -> 2168747664144
	2168747664144 [label=AccumulateGrad]
	2168747665104 -> 2168747664720
	2167409766128 [label="conv4.weight
 (256, 128, 4, 4)" fillcolor=lightblue]
	2167409766128 -> 2168747665104
	2168747665104 [label=AccumulateGrad]
	2168747664384 -> 2168747664720
	2167409766208 [label="conv4.bias
 (256)" fillcolor=lightblue]
	2167409766208 -> 2168747664384
	2168747664384 [label=AccumulateGrad]
	2168747664048 -> 2168747664672
	2168747664048 [label=TBackward0]
	2168747664768 -> 2168747664048
	2167075936272 [label="fc1.weight
 (512, 4096)" fillcolor=lightblue]
	2167075936272 -> 2168747664768
	2168747664768 [label=AccumulateGrad]
	2168747663904 -> 2168747664336
	2168747663904 [label=TBackward0]
	2168747664528 -> 2168747663904
	2167409788384 [label="fc_mu.weight
 (256, 512)" fillcolor=lightblue]
	2167409788384 -> 2168747664528
	2168747664528 [label=AccumulateGrad]
	2168747663664 -> 2168747663424
	2168747663664 [label=MulBackward0]
	2168747665008 -> 2168747663664
	2168747665008 [label=ExpBackward0]
	2168747664432 -> 2168747665008
	2168747664432 [label=MulBackward0]
	2168747665296 -> 2168747664432
	2168747665296 [label=AddmmBackward0]
	2168747665536 -> 2168747665296
	2167409789024 [label="fc_logvar.bias
 (256)" fillcolor=lightblue]
	2167409789024 -> 2168747665536
	2168747665536 [label=AccumulateGrad]
	2168747664000 -> 2168747665296
	2168747665152 -> 2168747665296
	2168747665152 [label=TBackward0]
	2168747665440 -> 2168747665152
	2167409790224 [label="fc_logvar.weight
 (256, 512)" fillcolor=lightblue]
	2167409790224 -> 2168747665440
	2168747665440 [label=AccumulateGrad]
	2168747664096 -> 2168747663472
	2168747664096 [label=TBackward0]
	2168747664864 -> 2168747664096
	2167409787104 [label="fc2.weight
 (4096, 256)" fillcolor=lightblue]
	2167409787104 -> 2168747664864
	2168747664864 [label=AccumulateGrad]
	2168221943120 -> 2168221944704
	2167409787344 [label="conv5.weight
 (256, 128, 4, 4)" fillcolor=lightblue]
	2167409787344 -> 2168221943120
	2168221943120 [label=AccumulateGrad]
	2168221942736 -> 2168221944704
	2167409787264 [label="conv5.bias
 (128)" fillcolor=lightblue]
	2167409787264 -> 2168221942736
	2168221942736 [label=AccumulateGrad]
	2168221943024 -> 2168221943360
	2167409789424 [label="conv6.weight
 (128, 64, 4, 4)" fillcolor=lightblue]
	2167409789424 -> 2168221943024
	2168221943024 [label=AccumulateGrad]
	2168221945376 -> 2168221943360
	2167409787584 [label="conv6.bias
 (64)" fillcolor=lightblue]
	2167409787584 -> 2168221945376
	2168221945376 [label=AccumulateGrad]
	2168221944416 -> 2168747560672
	2167409789984 [label="conv7.weight
 (64, 32, 4, 4)" fillcolor=lightblue]
	2167409789984 -> 2168221944416
	2168221944416 [label=AccumulateGrad]
	2168221945568 -> 2168747560672
	2167409789824 [label="conv7.bias
 (32)" fillcolor=lightblue]
	2167409789824 -> 2168221945568
	2168221945568 [label=AccumulateGrad]
	2168747557888 -> 2168747559712
	2167409787904 [label="conv8.weight
 (32, 1, 4, 4)" fillcolor=lightblue]
	2167409787904 -> 2168747557888
	2168747557888 [label=AccumulateGrad]
	2168747559856 -> 2168747559712
	2167409790704 [label="conv8.bias
 (1)" fillcolor=lightblue]
	2167409790704 -> 2168747559856
	2168747559856 [label=AccumulateGrad]
	2168747558608 -> 2169072070720
}
